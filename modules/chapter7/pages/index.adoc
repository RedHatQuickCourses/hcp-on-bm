=  Explore the Deployment

This lab explores the building blocks of the hosted cluster using the cluster deployed and how everything works under the hood.

== Explore The ControlPlane

1. For each Infrastructure Environment created, there will be a namespace created with the same name. 

.. Each discovered node will be available in that namespace as Agent.
+
[source,subs="verbatim,quotes"]
--
# oc get Agent -n hcpbm
NAME                                   CLUSTER   APPROVED   ROLE          STAGE
442b5079-f849-461a-af7d-0e74f584a9eb             true       auto-assign   
686107da-eef0-4ab5-a2ee-37bd8196760b             true       auto-assign  
--

.. More details can be obtained using -o yaml
+
[source,subs="verbatim,quotes"]
--
# oc get Agent -n hcpbm 442b5079-f849-461a-af7d-0e74f584a9eb -o yaml
apiVersion: agent-install.openshift.io/v1beta1
kind: Agent
....
....
--

2. For each hosted cluster, there will be three namespaces created using “cluster name” in the namespace name. Eg If the cluster name is “hcp-cluster1”
+
[source,subs="verbatim,quotes"]
--
# oc get ns | grep cluster1
hcp-cluster1                                       Active   66s
hcp-cluster1-hcp-cluster1                          Active   57s
klusterlet-hcp-cluster1                            Active   55s
--

3. The "hcp-cluster1" namespace.

.. It holds the HostedCluster CR that shows high level details of the cluster. -o yaml will show more details about the cluster configuration.
+
[source,subs="verbatim,quotes"]
--
# oc get HostedCluster -n hcp-cluster1
NAME           VERSION   KUBECONFIG                      PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
hcp-cluster1             hcp-cluster1-admin-kubeconfig   Partial    False       False         Waiting for Kube APIServer deployment to become available
--

.. Explore spec.etcd in the yaml output output.
+
[source,subs="verbatim,quotes"]
--
    etcd:
      managed:
        storage:
          persistentVolume:
            size: 8Gi
          type: PersistentVolume
      managementType: Managed
--

.. Most of the cluster details filled in the web UI during the cluster deployment can be found here.

.. This namespace also holds all the secrets associated with the hosted cluster. Eg pull secret, ssh key, kubeadmin password and kubeconfig
+
[source,subs="verbatim,quotes"]
--
# oc get secret -n hcp-cluster1
NAME                                                      TYPE                                  DATA   AGE
hcp-cluster1-admin-kubeconfig                             Opaque                                1      142m
hcp-cluster1-import                                       Opaque                                6      144m
hcp-cluster1-kubeadmin-password                           Opaque                                1      141m
open-cluster-management-compliance-history-api-recorder   kubernetes.io/service-account-token   4      144m
pullsecret-cluster-hcp-cluster1                           kubernetes.io/dockerconfigjson        1      144m
sshkey-cluster-hcp-cluster1                               Opaque                                1      144m
--

.. Explore the NodePool resource in this namespace. -o yam will show more details about the nodepool we created for the hosted cluster.
+
[source,subs="verbatim,quotes"]
--
# oc get NodePool -n hcp-cluster1
NAMESPACE      NAME                      CLUSTER        DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
hcp-cluster1   nodepool-hcp-cluster1-1   hcp-cluster1   2               2               False         False        4.16.43   False             False
--

4. The *clustername-clustername* namespace holds all the resources required for the hosted cluster. In this case namespace name “hcp-cluster1-hcp-cluster1”.

.. See details of various pods that run on this namespace that takes care of various control plane functions of the hosted cluster.
+
[source,subs="verbatim,quotes"]
--
#  oc get po -n hcp-cluster1-hcp-cluster1
NAME                                                  READY   STATUS    RESTARTS       AGE
capi-provider-5745984c97-8pxxd                        1/1     Running   1 (3m6s ago)   11m
catalog-operator-6c9c8575f5-r846n                     2/2     Running   0              7m37s
.....
.....
.....
.....
packageserver-77d56b97c5-zjng6                        2/2     Running   0              7m36s
redhat-marketplace-catalog-c446c5f89-7sc5w            1/1     Running   0              7m24s
redhat-operators-catalog-65c8c964bc-8pqnf             1/1     Running   0              7m24s
--

.. Explore the etcd pods.
+
[source,subs="verbatim,quotes"]
--
#  oc get po -n hcp-cluster1-hcp-cluster1| grep etcd
etcd-0                                                4/4     Running   0               11m
etcd-1                                                4/4     Running   0               11m
etcd-2                                                4/4     Running   0               11m
--
.. Explore the automatically created pvcs for etcd that comes from LVMStorage.
+
[source,subs="verbatim,quotes"]
--
# oc get pvc -n hcp-cluster1-hcp-cluster1
NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
data-etcd-0   Bound    pvc-8e6967a1-b398-4794-b777-7d27dc680b8c   8Gi        RWO            lvms-vg1       <unset>                 12m
data-etcd-1   Bound    pvc-86cc384a-71d6-4443-9ec2-9b74dcc254cd   8Gi        RWO            lvms-vg1       <unset>                 12m
data-etcd-2   Bound    pvc-1831e4af-b73f-4d3e-96bd-625b445ebc78   8Gi        RWO            lvms-vg1       <unset>                 12m

# oc get pv | grep etcd
pvc-1831e4af-b73f-4d3e-96bd-625b445ebc78   8Gi        RWO            Delete           Bound    hcp-cluster1-hcp-cluster1/data-etcd-2                             lvms-vg1       <unset>                          14m
pvc-86cc384a-71d6-4443-9ec2-9b74dcc254cd   8Gi        RWO            Delete           Bound    hcp-cluster1-hcp-cluster1/data-etcd-1                             lvms-vg1       <unset>                          14m
pvc-8e6967a1-b398-4794-b777-7d27dc680b8c   8Gi        RWO            Delete           Bound    hcp-cluster1-hcp-cluster1/data-etcd-0                             lvms-vg1       <unset>                          14m
--

.. The oauth, konnectivity and ignition will be listening on a route on the hub cluster. Explore the routes created.
+
[source,subs="verbatim,quotes"]
--
# oc get route -n hcp-cluster1-hcp-cluster1
NAME                  HOST/PORT                                                          PATH   SERVICES                PORT    TERMINATION        WILDCARD
ignition-server       ignition-server-hcp-cluster1-hcp-cluster1.apps.hub.mylab.com              ignition-server-proxy   <all>   passthrough/None   None
konnectivity-server   konnectivity-server-hcp-cluster1-hcp-cluster1.apps.hub.mylab.com          konnectivity-server     <all>   passthrough/None   None
oauth                 oauth-hcp-cluster1-hcp-cluster1.apps.hub.mylab.com                        oauth-openshift         <all>   passthrough/None   None
--

.. Explore svc associated with each route.
+
[source,subs="verbatim,quotes"]
--
oc get route -n hcp-cluster1-hcp-cluster1 oauth 
NAME    HOST/PORT                                            PATH   SERVICES          PORT    TERMINATION        WILDCARD
oauth   oauth-hcp-cluster1-hcp-cluster1.apps.hub.mylab.com          oauth-openshift   <all>   passthrough/None   None

-o yaml

spec:
  host: oauth-hcp-cluster1-hcp-cluster1.apps.hub.mylab.com
  tls:
    insecureEdgeTerminationPolicy: None
    termination: passthrough
  to:
    kind: Service
    name: oauth-openshift
    weight: 100
  wildcardPolicy: None
--

.. Route points to svc “oauth-openshift” which maps to pods with label app: “oauth-openshift”
+
[source,subs="verbatim,quotes"]
--
# oc get svc -n hcp-cluster1-hcp-cluster1  oauth-openshift -o yaml
  ....
  selector:
    app: oauth-openshift
    hypershift.openshift.io/control-plane-component: oauth-openshift
--

.. They are the pods running on the hosted cluster ns with name “oauth-openshift”. So any attempt to access the oauth route will hit the pods in the hosted cluster namespace.
+
[source,subs="verbatim,quotes"]
--
# oc get po -n hcp-cluster1-hcp-cluster1 -l app=oauth-openshift
NAME                               READY   STATUS    RESTARTS   AGE
oauth-openshift-6fc8bbc966-422rl   4/4     Running   0          64m
oauth-openshift-6fc8bbc966-84mrv   4/4     Running   0          64m
oauth-openshift-6fc8bbc966-xnxkw   4/4     Running   0          64m
--

.. Explore the Konnectivity route and Ignition route in a similar manner.

.. Explore the AgentCluster resource in this namespace
+
[source,subs="verbatim,quotes"]
--
# oc get AgentCluster -n hcp-cluster1-hcp-cluster1
NAME           AGE
hcp-cluster1   146m

[root@ip-10-0-14-2 ~]# oc get AgentCluster -n hcp-cluster1-hcp-cluster1 -o yaml
apiVersion: v1
items:
- apiVersion: capi-provider.agent-install.openshift.io/v1beta1
....
....
--

== API Access

1. Below diagram shows how API access to the hosted cluster works.
+
image::fig-1.jpeg[]

.. DNS resolves api.hcp-cluster1.mylab.com to 192.168.122.60

.. IP address 192.168.122.60 is configured by MetalLB to listen on port 6443 during hosted cluster creation.

.. The metal LB range is configured to use 192.168.122.60 - 192.168.122.69. We knew it takes the first ip for the first cluster and second ip for the second cluster. That is why we configured DNS for hcp-cluster1 to use 192.168.122.60 and hcp-cluster2 to use 192.168.122.61.

.. There isn’t a better way to predict this at this time. More improvements like dynamic integration with DNS and annotation to get predictable ip to HostedCluster CR is work in progress at this time.
+
[source,subs="verbatim,quotes"]
--
# oc get IPAddressPool -n metallb-system
NAME          AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES
hcp-ip-pool   true          true              ["192.168.122.60-192.168.122.69"]
--

.. MeatlLB will listen for that ip on one of the master nodes on the  hub cluster. You can confirm this via rping. Look at the mac address that ip listens on
+
[source,subs="verbatim,quotes"]
--
# arping 192.168.122.60 -I virbr0
ARPING 192.168.122.60 from 192.168.122.1 virbr0
Unicast reply from 192.168.122.60 [52:54:00:E2:54:32]  6.625ms
--

.. The hosted cluster namespace has a LoadBalancer TYPE svc created that forwards a request to 192.168.122.60:6443 to a nodeport. Which goes to kube-apiserver pods where the API for hosted cluster runs.
+
[source,subs="verbatim,quotes"]
--
# oc get svc -n hcp-cluster1-hcp-cluster1 kube-apiserver
NAME             TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)          AGE
kube-apiserver   LoadBalancer   172.30.168.59   192.168.122.60   6443:30447/TCP   105m

-o yaml view.

 selector:
    app: kube-apiserver
    hypershift.openshift.io/control-plane-component: kube-apiserver

# oc get po -n hcp-cluster1-hcp-cluster1 -l app=kube-apiserver
NAME                              READY   STATUS    RESTARTS   AGE
kube-apiserver-69794fffb6-4mgrp   4/4     Running   0          108m
kube-apiserver-69794fffb6-5wbpm   4/4     Running   0          108m
kube-apiserver-69794fffb6-dwl2x   4/4     Running   0          108m
--

== Explore Hosted Cluster

1. Login to the hosted cluster API to explore some details. Passwords can be obtained from webconsole.
+
[source,subs="verbatim,quotes"]
--
oc login https://api.hcp-cluster1.mylab.com:6443 -u kubeadmin
--

2. Explore the pods running on the kube-system namespace in the hosted cluster.
+
[source,subs="verbatim,quotes"]
--
# oc get po -n kube-system
NAME                             READY   STATUS    RESTARTS   AGE
konnectivity-agent-25r69         1/1     Running   0          111m
konnectivity-agent-drmg7         1/1     Running   0          111m
kube-apiserver-proxy-c1worker1   1/1     Running   0          109m
kube-apiserver-proxy-c1worker2   1/1     Running   0          109m
--

3. Each worker node will be running a konnectivity pod and kube-apiservr-proxy pod.

4. The konnectivity pod runs the konnectivity proxy-agent that opens a tunnel from each worker node to the control plane. This enables bidirectional communication between each worker node and control plane after the tunnel is created.

+
[source,subs="verbatim,quotes"]
--
# oc project kube-system
Now using project "kube-system" on server "https://api.hcp-cluster1.mylab.com:6443".

[root@ip-10-0-14-2 ~]# oc rsh konnectivity-agent-25r69

sh-5.1$ ps aux | grep konnectivity
1000           1  0.1  0.2 1763736 24344 ?       Ssl  05:05   0:09 /usr/bin/proxy-agent --logtostderr=true --ca-cert /etc/konnectivity/ca/ca.crt --agent-cert /etc/konnectivity/agent/tls.crt --agent-key /etc/konnectivity/agent/tls.key --proxy-server-host konnectivity-server-hcp-cluster1-hcp-cluster1.apps.hub.mylab.com --proxy-server-port 443 --health-server-port 2041 --agent-identifiers=default-route=true --keepalive-time 30s --probe-interval 5s --sync-interval 5s --sync-interval-cap 30s --v 3
--

5. Note the URL --proxy-server-host konnectivity-server-hcp-cluster1-hcp-cluster1.apps.hub.mylab.com in the above output. This URL is a route running on the hub cluster ingress and forwards requests to the konnectivity pods running on hosing/hub cluster.

6. The kube-apiserver-proxy-xxx pod enables pods on each  worker node to access the hosted cluster API without going out to the hub cluster. Only this pod needs access to the API running on the hub cluster. Other pods can access API via this pod.

7. This pod runs haproxy where haproxy listens for API requests on ip 172.20.0.1:6443 internally and then proxy to remote_apiserver which is api.hcp-cluster1.mylab.com. Other pods can access the API using 172.20.0.1:6443 which this pod will forward to hub cluster.
+
[source,subs="verbatim,quotes"]
--
# oc rsh kube-apiserver-proxy-c1worker1

sh-5.1$ ps aux | grep ha
1001           1  0.1  0.1 240048 13260 ?        Ssl  05:04   0:11 haproxy -f /usr/local/etc/haproxy

sh-5.1$ cat /usr/local/etc/haproxy/haproxy.cfg 
global
  maxconn 7000
  log stdout local0
  log stdout local1 notice
defaults
  mode tcp
  timeout client 10m
  timeout server 10m
  timeout connect 10s
  timeout client-fin 5s
  timeout server-fin 5s
  timeout queue 5s
  retries 3
frontend local_apiserver
  bind 172.20.0.1:6443
  log global
  mode tcp
  option tcplog
  default_backend remote_apiserver
backend remote_apiserver
  mode tcp
  log global
  option httpchk GET /version
  option log-health-checks
  default-server inter 10s fall 3 rise 3
  server controlplane api.hcp-cluster1.mylab.com:6443
--

== Ingress for Hosted Cluster

1. The ingress pods are running on the hosted cluster worker nodes and are routed directly to those pods by an external load balancer. Ingress for the application running on a hosted cluster will not go to the hub cluster.

2. You can see console-openshift-console.apps.hcp-cluster1.mylab.com resolves to ip 192.168.122.49
+
[source,subs="verbatim,quotes"]
--
# nslookup console-openshift-console.apps.hcp-cluster1.mylab.com
Server:		192.168.122.21
Address:	192.168.122.21#53
Name:	console-openshift-console.apps.hcp-cluster1.mylab.com
Address: 192.168.122.49
--

3. Exploring /etc/haproxy/haproxy.cfg on helper vm show that anything that comes to 443 port of 192.168.122.49 is sent to one of the worker nodes for the hosted cluster.
+
[source,subs="verbatim,quotes"]
--
#------------------- HCP-Cluster1 APPS - HTTPS  ----------------------------#
frontend hcp_cluster1_apps_https
    bind  192.168.122.49:443
    use_backend hcp_cluster1_apps_https_backend
    mode tcp 
backend hcp_cluster1_apps_https_backend
    balance roundrobin
    mode tcp 
    server worker1 192.168.122.41:443 check
    server worker2 192.168.122.42:443 check 
    server worker3 192.168.122.43:443 check
--

4. The ingress pods will be running on the worker nodes just like any openshift nodes.
+
[source,subs="verbatim,quotes"]
--
# oc get po -n openshift-ingress
NAME                              READY   STATUS    RESTARTS   AGE
router-default-569b6445f5-fvd2g   1/1     Running   0          121m
router-default-569b6445f5-vz8ss   1/1     Running   0          121m
--

5. Note that though the openshift console is accessed through hosted cluster ingress and console pods are running on hosted cluster worker nodes, the end user still needs access to hub cluster ingress since oauth is running on hub cluster ingress. The redirection to oauth will get broken if end user is not having access to hub cluster ingress.